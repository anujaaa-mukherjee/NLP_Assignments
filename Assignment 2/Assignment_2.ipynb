{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8rYMTI-b8G2",
        "outputId": "2c07f7a2-4ce8-4b07-cc4b-9b4e1077a94f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vVc65lrTb8EX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"Cats and dogs are great\"\n",
        "]"
      ],
      "metadata": {
        "id": "unyuUwgecKLV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Count Occurrence ---\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "tokens = vectorizer.get_feature_names_out()\n",
        "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=tokens)"
      ],
      "metadata": {
        "id": "4nwsQ0WkcKIl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Bag of Words (Count Occurrence) ---\")\n",
        "print(df_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haVJsmdRcKAd",
        "outputId": "8cfbec80-961e-4d0d-9525-1c7762c14744"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Bag of Words (Count Occurrence) ---\n",
            "   and  are  cat  cats  dog  dogs  great  log  mat  on  sat  the\n",
            "0    0    0    1     0    0     0      0    0    1   1    1    2\n",
            "1    0    0    0     0    1     0      0    1    0   1    1    2\n",
            "2    1    1    0     1    0     1      1    0    0   0    0    0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Normalized Count Occurrence ---\n",
        "# We divide each row by the sum of its elements\n",
        "bow_array = bow_matrix.toarray()\n",
        "normalized_bow = bow_array / bow_array.sum(axis=1)[:, None]\n",
        "\n",
        "df_norm_bow = pd.DataFrame(normalized_bow, columns=tokens)"
      ],
      "metadata": {
        "id": "ZX90TM1hcQD9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Normalized Bag of Words ---\")\n",
        "print(df_norm_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgGUEaS1cQB-",
        "outputId": "aa87d68f-e4da-4400-b630-62bd25382e2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Normalized Bag of Words ---\n",
            "   and  are       cat  cats       dog  dogs  great       log       mat  \\\n",
            "0  0.0  0.0  0.166667   0.0  0.000000   0.0    0.0  0.000000  0.166667   \n",
            "1  0.0  0.0  0.000000   0.0  0.166667   0.0    0.0  0.166667  0.000000   \n",
            "2  0.2  0.2  0.000000   0.2  0.000000   0.2    0.2  0.000000  0.000000   \n",
            "\n",
            "         on       sat       the  \n",
            "0  0.166667  0.166667  0.333333  \n",
            "1  0.166667  0.166667  0.333333  \n",
            "2  0.000000  0.000000  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CplJpWaXcXy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\n--- TF-IDF Matrix ---\")\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZFQeAjob8Bu",
        "outputId": "b9b4890a-a75c-4e85-936c-3a7f59db3e2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TF-IDF Matrix ---\n",
            "        and       are       cat      cats       dog      dogs     great  \\\n",
            "0  0.000000  0.000000  0.427554  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.427554  0.000000  0.000000   \n",
            "2  0.447214  0.447214  0.000000  0.447214  0.000000  0.447214  0.447214   \n",
            "\n",
            "        log       mat        on       sat       the  \n",
            "0  0.000000  0.427554  0.325166  0.325166  0.650331  \n",
            "1  0.427554  0.000000  0.325166  0.325166  0.650331  \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "7EWzZnHRb7_O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "3J0nL17Gb78f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1X5HBesbwy1",
        "outputId": "f776aaa1-118d-4fef-a4f0-5abe5caf461a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Add these downloads at the top of your script\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # This is the missing resource causing your error\n",
        "\n",
        "data = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the log\",\n",
        "    \"cats and dogs are great pets\",\n",
        "    \"i love my cat and my dog\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: Word2Vec expects a list of lists (tokenized sentences)\n",
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in data]\n",
        "\n",
        "# Initialize and Train Model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "QzeIZNzvcc7u"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenized Data:\", tokenized_data)\n",
        "print(\"Vector for 'cat':\", model.wv['cat'][:5]) # Displaying first 5 dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JZOwzoOdHIs",
        "outputId": "c8866039-26e5-4f6c-92f2-c3deda4c38bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Data: [['the', 'cat', 'sat', 'on', 'the', 'mat'], ['the', 'dog', 'sat', 'on', 'the', 'log'], ['cats', 'and', 'dogs', 'are', 'great', 'pets'], ['i', 'love', 'my', 'cat', 'and', 'my', 'dog']]\n",
            "Vector for 'cat': [ 0.00813227 -0.00445733 -0.00106836  0.00100636 -0.00019111]\n"
          ]
        }
      ]
    }
  ]
}