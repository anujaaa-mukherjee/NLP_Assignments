{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVSm6psqd8CM",
        "outputId": "a8cf4969-5ed9-425f-8d4e-c2312fd4aef3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "_2QOnFUEd7_8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup & Downloads\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oQIpUGpd79t",
        "outputId": "e6b9a0ee-d0e1-4051-94d9-a132a32d574c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Dataset: Text and its Category (Label)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"The cats are running around the house!\",\n",
        "        \"Dogs love playing in the park with a ball.\",\n",
        "        \"I am eating a delicious apple for breakfast.\",\n",
        "        \"The software engineers are building great programs.\",\n",
        "        \"Apples and oranges are healthy fruits.\"\n",
        "    ],\n",
        "    'category': ['animal', 'animal', 'food', 'tech', 'food']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "TkABa-7Md77c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Text Cleaning, Stopword Removal, and Lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation and special characters, keep only letters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    cleaned_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "    return \" \".join(cleaned_words)\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "p-SF8X4-d749"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "frwQKwVVd3a2"
      },
      "outputs": [],
      "source": [
        "# 3. Label Encoding (Categorical to Numerical)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['category'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. TF-IDF Representation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Create a DataFrame for TF-IDF output\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "aemQWkB1eBQD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Display and Save Outputs\n",
        "print(\"--- Processed Data ---\")\n",
        "print(df[['text', 'cleaned_text', 'label_encoded']])\n",
        "\n",
        "print(\"\\n--- TF-IDF Matrix (Snippet) ---\")\n",
        "print(tfidf_df.iloc[:, :5]) # Showing first 5 columns\n",
        "\n",
        "# Saving outputs\n",
        "df.to_csv(\"processed_text_data.csv\", index=False)\n",
        "tfidf_df.to_csv(\"tfidf_features.csv\", index=False)\n",
        "\n",
        "print(\"\\nSuccess: Outputs saved to 'processed_text_data.csv' and 'tfidf_features.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikiN7voMeBNV",
        "outputId": "d13f3331-e75d-418f-bc00-e8f4624d364b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processed Data ---\n",
            "                                                text  \\\n",
            "0             The cats are running around the house!   \n",
            "1         Dogs love playing in the park with a ball.   \n",
            "2       I am eating a delicious apple for breakfast.   \n",
            "3  The software engineers are building great prog...   \n",
            "4             Apples and oranges are healthy fruits.   \n",
            "\n",
            "                               cleaned_text  label_encoded  \n",
            "0                  cat running around house              0  \n",
            "1                dog love playing park ball              0  \n",
            "2          eating delicious apple breakfast              1  \n",
            "3  software engineer building great program              2  \n",
            "4                apple orange healthy fruit              1  \n",
            "\n",
            "--- TF-IDF Matrix (Snippet) ---\n",
            "      apple  around      ball  breakfast  building\n",
            "0  0.000000     0.5  0.000000   0.000000  0.000000\n",
            "1  0.000000     0.0  0.447214   0.000000  0.000000\n",
            "2  0.422242     0.0  0.000000   0.523358  0.000000\n",
            "3  0.000000     0.0  0.000000   0.000000  0.447214\n",
            "4  0.422242     0.0  0.000000   0.000000  0.000000\n",
            "\n",
            "Success: Outputs saved to 'processed_text_data.csv' and 'tfidf_features.csv'\n"
          ]
        }
      ]
    }
  ]
}