{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (\n",
        "    WhitespaceTokenizer,\n",
        "    WordPunctTokenizer,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "qKcW77jHaH0A"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPdb4R46aHxJ",
        "outputId": "a21d2dbd-6950-40e6-d0d7-5d11976f6fef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_nlp_tasks(sample_text):\n",
        "\n",
        "\n",
        "    print(\"--- 1. TOKENIZATION ---\")\n",
        "\n",
        "    # A. Whitespace Tokenization\n",
        "    ws_tokenizer = WhitespaceTokenizer()\n",
        "    print(f\"Whitespace: {ws_tokenizer.tokenize(sample_text)}\")\n",
        "\n",
        "    # B. Punctuation-based Tokenization\n",
        "    punct_tokenizer = WordPunctTokenizer()\n",
        "    print(f\"Punctuation-based: {punct_tokenizer.tokenize(sample_text)}\")\n",
        "\n",
        "    # C. Treebank Tokenization\n",
        "    tree_tokenizer = TreebankWordTokenizer()\n",
        "    print(f\"Treebank: {tree_tokenizer.tokenize(sample_text)}\")\n",
        "\n",
        "    # D. Tweet Tokenization\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    print(f\"Tweet: {tweet_tokenizer.tokenize(sample_text)}\")\n",
        "\n",
        "    # E. Multi-Word Expression (MWE) Tokenization\n",
        "    mwe_tokenizer = MWETokenizer()\n",
        "    mwe_tokenizer.add_mwe(('Python', 'programs.'))\n",
        "    # MWE tokenizer requires text already split into tokens\n",
        "    mwe_tokens = mwe_tokenizer.tokenize(sample_text.split())\n",
        "    print(f\"MWE (Python programs.): {mwe_tokens}\")\n",
        "\n",
        "    print(\"\\n--- 2. STEMMING ---\")\n",
        "    words = [\"running\", \"flies\", \"happily\", \"denied\", \"better\"]\n",
        "\n",
        "    porter = PorterStemmer()\n",
        "    snowball = SnowballStemmer(language='english')\n",
        "\n",
        "    print(f\"{'Word':<10} | {'Porter':<12} | {'Snowball'}\")\n",
        "    print(\"-\" * 35)\n",
        "    for w in words:\n",
        "        print(f\"{w:<10} | {porter.stem(w):<12} | {snowball.stem(w)}\")\n",
        "\n",
        "    print(\"\\n--- 3. LEMMATIZATION ---\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lem_words = [\"feet\", \"corpora\", \"rocks\", \"better\"]\n",
        "\n",
        "    for w in lem_words:\n",
        "        # 'v' for verb, 'n' for noun, 'a' for adjective\n",
        "        # If 'better' is treated as an adjective:\n",
        "        pos = 'a' if w == 'better' else 'n'\n",
        "        print(f\"{w} -> {lemmatizer.lemmatize(w, pos=pos)}\")"
      ],
      "metadata": {
        "id": "61nXE0OUaHun"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sample_text = \"NLTK is a leading platform for building Python programs. It's great for #NLP! @NLTK_org\"\n",
        "    perform_nlp_tasks(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGPuF8RlaHsP",
        "outputId": "cadf8955-1d06-4184-ca89-a4b686c0b048"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. TOKENIZATION ---\n",
            "Whitespace: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs.', \"It's\", 'great', 'for', '#NLP!', '@NLTK_org']\n",
            "Punctuation-based: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', '.', 'It', \"'\", 's', 'great', 'for', '#', 'NLP', '!', '@', 'NLTK_org']\n",
            "Treebank: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs.', 'It', \"'s\", 'great', 'for', '#', 'NLP', '!', '@', 'NLTK_org']\n",
            "Tweet: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', '.', \"It's\", 'great', 'for', '#NLP', '!', '@NLTK_org']\n",
            "MWE (Python programs.): ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python_programs.', \"It's\", 'great', 'for', '#NLP!', '@NLTK_org']\n",
            "\n",
            "--- 2. STEMMING ---\n",
            "Word       | Porter       | Snowball\n",
            "-----------------------------------\n",
            "running    | run          | run\n",
            "flies      | fli          | fli\n",
            "happily    | happili      | happili\n",
            "denied     | deni         | deni\n",
            "better     | better       | better\n",
            "\n",
            "--- 3. LEMMATIZATION ---\n",
            "feet -> foot\n",
            "corpora -> corpus\n",
            "rocks -> rock\n",
            "better -> good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JY1UNFG_aHpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5r_IkfiaEov"
      },
      "outputs": [],
      "source": []
    }
  ]
}